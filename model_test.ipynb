{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904648a2",
   "metadata": {},
   "source": [
    "# ONNX Model Parameter Counter\n",
    "\n",
    "This notebook analyzes ONNX models to count the number of parameters and provide detailed breakdowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01910b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\mouse\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# # Install and import required packages for ONNX analysis\n",
    "# import subprocess\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import onnx\n",
    "# import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38ae799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54817b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_onnx_parameters(model_path):\n",
    "    \"\"\"\n",
    "    Count the number of parameters in an ONNX model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the .onnx model file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with parameter counts and details\n",
    "    \"\"\"\n",
    "    # Load the ONNX model\n",
    "    model = onnx.load(model_path)\n",
    "    \n",
    "    # Initialize counters\n",
    "    total_params = 0\n",
    "    layer_details = []\n",
    "    \n",
    "    # Create a mapping for data types (compatible with different ONNX versions)\n",
    "    def get_dtype_name(data_type):\n",
    "        try:\n",
    "            # Try new ONNX version first\n",
    "            if hasattr(onnx, 'mapping') and hasattr(onnx.mapping, 'TENSOR_TYPE_TO_NP_TYPE'):\n",
    "                return onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[data_type].__name__\n",
    "            else:\n",
    "                # Fallback for newer ONNX versions\n",
    "                dtype_map = {\n",
    "                    1: 'float32',  # FLOAT\n",
    "                    2: 'uint8',    # UINT8\n",
    "                    3: 'int8',     # INT8\n",
    "                    4: 'uint16',   # UINT16\n",
    "                    5: 'int16',    # INT16\n",
    "                    6: 'int32',    # INT32\n",
    "                    7: 'int64',    # INT64\n",
    "                    8: 'str',      # STRING\n",
    "                    9: 'bool',     # BOOL\n",
    "                    10: 'float16', # FLOAT16\n",
    "                    11: 'float64', # DOUBLE\n",
    "                    12: 'uint32',  # UINT32\n",
    "                    13: 'uint64',  # UINT64\n",
    "                }\n",
    "                return dtype_map.get(data_type, f'unknown_type_{data_type}')\n",
    "        except:\n",
    "            return f'type_{data_type}'\n",
    "    \n",
    "    # Get all initializers (weights and biases)\n",
    "    for initializer in model.graph.initializer:\n",
    "        # Get the shape of the tensor\n",
    "        shape = [dim for dim in initializer.dims]\n",
    "        \n",
    "        # Calculate number of parameters in this tensor\n",
    "        if shape:\n",
    "            num_params = np.prod(shape)\n",
    "        else:\n",
    "            num_params = 1\n",
    "            \n",
    "        total_params += num_params\n",
    "        \n",
    "        # Store details\n",
    "        layer_details.append({\n",
    "            'name': initializer.name,\n",
    "            'shape': shape,\n",
    "            'params': num_params,\n",
    "            'data_type': get_dtype_name(initializer.data_type)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'total_parameters': total_params,\n",
    "        'layer_details': layer_details,\n",
    "        'num_layers': len(layer_details)\n",
    "    }\n",
    "\n",
    "def print_model_summary(model_path):\n",
    "    \"\"\"Print a nice summary of the ONNX model parameters\"\"\"\n",
    "    print(f\"üîç Analyzing ONNX model: {model_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        result = count_onnx_parameters(model_path)\n",
    "        \n",
    "        print(f\"üìä PARAMETER SUMMARY:\")\n",
    "        print(f\"   Total Parameters: {result['total_parameters']:,}\")\n",
    "        print(f\"   Number of Layers: {result['num_layers']}\")\n",
    "        print(f\"   Model Size: ~{result['total_parameters'] * 4 / 1024 / 1024:.2f} MB (assuming float32)\")\n",
    "        \n",
    "        print(f\"\\nüìã LAYER BREAKDOWN:\")\n",
    "        print(f\"{'Layer Name':<30} {'Shape':<20} {'Parameters':<12} {'Type'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for layer in result['layer_details']:\n",
    "            shape_str = str(layer['shape'])\n",
    "            print(f\"{layer['name']:<30} {shape_str:<20} {layer['params']:<12,} {layer['data_type']}\")\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "356a3dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found model: results/nature_cnn_NormalTrain_4/My Behavior.onnx\n",
      "üîç Analyzing ONNX model: results/nature_cnn_NormalTrain_4/My Behavior.onnx\n",
      "============================================================\n",
      "üìä PARAMETER SUMMARY:\n",
      "   Total Parameters: 7,622,728\n",
      "   Number of Layers: 17\n",
      "   Model Size: ~29.08 MB (assuming float32)\n",
      "\n",
      "üìã LAYER BREAKDOWN:\n",
      "Layer Name                     Shape                Parameters   Type\n",
      "--------------------------------------------------------------------------------\n",
      "version_number.1               [1]                  1            float32\n",
      "memory_size_vector             [1]                  1            float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.0.weight [64, 1, 6, 6]        2,304        float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.0.bias [64]                 64           float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.2.weight [128, 64, 4, 4]      131,072      float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.2.bias [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.4.weight [128, 128, 3, 3]     147,456      float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.4.bias [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.dense.0.weight [256, 28160]         7,208,960    float32\n",
      "network_body.observation_encoder.processors.0.dense.0.bias [256]                256          float32\n",
      "network_body._body_endoder.seq_layers.0.weight [256, 256]           65,536       float32\n",
      "network_body._body_endoder.seq_layers.0.bias [256]                256          float32\n",
      "network_body._body_endoder.seq_layers.2.weight [256, 256]           65,536       float32\n",
      "network_body._body_endoder.seq_layers.2.bias [256]                256          float32\n",
      "action_model._continuous_distribution.log_sigma [1, 3]               3            float32\n",
      "action_model._continuous_distribution.mu.weight [3, 256]             768          float32\n",
      "action_model._continuous_distribution.mu.bias [3]                  3            float32\n"
     ]
    }
   ],
   "source": [
    "# Example: Analyze your example model\n",
    "model_path = \"results/nature_cnn_NormalTrain_4/My Behavior.onnx\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"‚úì Found model: {model_path}\")\n",
    "    result = print_model_summary(model_path)\n",
    "else:\n",
    "    print(f\"‚ùå Model not found: {model_path}\")\n",
    "    print(\"\\nüí° Let's check what ONNX models are available:\")\n",
    "    \n",
    "    # Check current directory\n",
    "    current_dir_models = [f for f in os.listdir('.') if f.endswith('.onnx')]\n",
    "    if current_dir_models:\n",
    "        print(\"üìÅ Current directory:\")\n",
    "        for model in current_dir_models:\n",
    "            print(f\"   - {model}\")\n",
    "    \n",
    "    # Check results directory for trained models\n",
    "    import glob\n",
    "    results_models = glob.glob(\"./results/*/*.onnx\")\n",
    "    if results_models:\n",
    "        print(\"\\nüìÅ Trained models in results/:\")\n",
    "        for model in results_models[:5]:  # Show first 5\n",
    "            print(f\"   - {model}\")\n",
    "        if len(results_models) > 5:\n",
    "            print(f\"   ... and {len(results_models) - 5} more\")\n",
    "    \n",
    "    if not current_dir_models and not results_models:\n",
    "        print(\"   No ONNX models found.\")\n",
    "        print(\"   Train some models first using: python train.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c00dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Analyzing specific model...\n",
      "üîç Analyzing ONNX model: results/nature_cnn_NormalTrain_4/My Behavior.onnx\n",
      "============================================================\n",
      "üìä PARAMETER SUMMARY:\n",
      "   Total Parameters: 7,622,728\n",
      "   Number of Layers: 17\n",
      "   Model Size: ~29.08 MB (assuming float32)\n",
      "\n",
      "üìã LAYER BREAKDOWN:\n",
      "Layer Name                     Shape                Parameters   Type\n",
      "--------------------------------------------------------------------------------\n",
      "version_number.1               [1]                  1            float32\n",
      "memory_size_vector             [1]                  1            float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.0.weight [64, 1, 6, 6]        2,304        float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.0.bias [64]                 64           float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.2.weight [128, 64, 4, 4]      131,072      float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.2.bias [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.4.weight [128, 128, 3, 3]     147,456      float32\n",
      "network_body.observation_encoder.processors.0.conv_layers.4.bias [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.dense.0.weight [256, 28160]         7,208,960    float32\n",
      "network_body.observation_encoder.processors.0.dense.0.bias [256]                256          float32\n",
      "network_body._body_endoder.seq_layers.0.weight [256, 256]           65,536       float32\n",
      "network_body._body_endoder.seq_layers.0.bias [256]                256          float32\n",
      "network_body._body_endoder.seq_layers.2.weight [256, 256]           65,536       float32\n",
      "network_body._body_endoder.seq_layers.2.bias [256]                256          float32\n",
      "action_model._continuous_distribution.log_sigma [1, 3]               3            float32\n",
      "action_model._continuous_distribution.mu.weight [3, 256]             768          float32\n",
      "action_model._continuous_distribution.mu.bias [3]                  3            float32\n"
     ]
    }
   ],
   "source": [
    "# Analyze specific model by path\n",
    "# Replace with your actual model path\n",
    "specific_model = \"results/nature_cnn_NormalTrain_4/My Behavior.onnx\"  # Example path\n",
    "\n",
    "print(\"üéØ Analyzing specific model...\")\n",
    "if os.path.exists(specific_model):\n",
    "    result = print_model_summary(specific_model)\n",
    "else:\n",
    "    print(f\"Model not found: {specific_model}\")\n",
    "    print(\"Update the path above to point to your trained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6797026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Analyzing specific model...\n",
      "üîç Analyzing ONNX model: results/transcoder3_NormalTrain_1/My Behavior.onnx\n",
      "============================================================\n",
      "üìä PARAMETER SUMMARY:\n",
      "   Total Parameters: 11,831,944\n",
      "   Number of Layers: 22\n",
      "   Model Size: ~45.14 MB (assuming float32)\n",
      "\n",
      "üìã LAYER BREAKDOWN:\n",
      "Layer Name                     Shape                Parameters   Type\n",
      "--------------------------------------------------------------------------------\n",
      "version_number.1               [1]                  1            float32\n",
      "memory_size_vector             [1]                  1            float32\n",
      "network_body.observation_encoder.processors.0.patch_embeddings.weight [128, 1, 6, 6]       4,608        float32\n",
      "network_body.observation_encoder.processors.0.position_embeddings.weight [350, 128]           44,800       float32\n",
      "network_body.observation_encoder.processors.0.layer_norm.weight [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.layer_norm.bias [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.mlp.0.bias [512]                512          float32\n",
      "network_body.observation_encoder.processors.0.mlp.2.bias [128]                128          float32\n",
      "network_body.observation_encoder.processors.0.dense.0.weight [256, 44800]         11,468,800   float32\n",
      "network_body.observation_encoder.processors.0.dense.0.bias [256]                256          float32\n",
      "network_body._body_endoder.seq_layers.0.weight [256, 256]           65,536       float32\n",
      "network_body._body_endoder.seq_layers.0.bias [256]                256          float32\n",
      "network_body._body_endoder.seq_layers.2.weight [256, 256]           65,536       float32\n",
      "network_body._body_endoder.seq_layers.2.bias [256]                256          float32\n",
      "action_model._continuous_distribution.log_sigma [1, 3]               3            float32\n",
      "action_model._continuous_distribution.mu.weight [3, 256]             768          float32\n",
      "action_model._continuous_distribution.mu.bias [3]                  3            float32\n",
      "onnx::MatMul_123               [128, 128]           16,384       float32\n",
      "onnx::MatMul_124               [128, 128]           16,384       float32\n",
      "onnx::MatMul_125               [128, 128]           16,384       float32\n",
      "onnx::MatMul_126               [128, 512]           65,536       float32\n",
      "onnx::MatMul_127               [512, 128]           65,536       float32\n"
     ]
    }
   ],
   "source": [
    "# Analyze specific model by path\n",
    "# Replace with your actual model path\n",
    "specific_model = \"results/transcoder3_NormalTrain_1/My Behavior.onnx\"  # Example path\n",
    "\n",
    "print(\"üéØ Analyzing specific model...\")\n",
    "if os.path.exists(specific_model):\n",
    "    result = print_model_summary(specific_model)\n",
    "else:\n",
    "    print(f\"Model not found: {specific_model}\")\n",
    "    print(\"Update the path above to point to your trained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "035e5249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 867,424 (~3.5 MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LightweightTranscoder3(nn.Module):\n",
    "    def __init__(self, height: int, width: int, initial_channels: int, output_size: int):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = 12  # Larger for fewer patches\n",
    "        self.embed_size = 32   # Reduced from 128\n",
    "        self.head_size = 32    # Matches embed_size for single-head\n",
    "\n",
    "        # Patch grid: 155//12=12 (pad if needed), but exact 13x7=91 for 155/86\n",
    "        self.num_patches_h = (height + self.kernel_size - 1) // self.kernel_size  # Ceiling div for padding\n",
    "        self.num_patches_w = (width + self.kernel_size - 1) // self.kernel_size\n",
    "        self.num_patches = self.num_patches_h * self.num_patches_w  # ~91\n",
    "\n",
    "        # Lightweight patch embedding (small conv)\n",
    "        self.patch_embeddings = nn.Conv2d(\n",
    "            initial_channels, self.embed_size,\n",
    "            kernel_size=self.kernel_size, stride=self.kernel_size,\n",
    "            bias=False  # Save params\n",
    "        )\n",
    "\n",
    "        # Positional embeddings (now small)\n",
    "        self.position_embeddings = nn.Embedding(self.num_patches, self.embed_size)\n",
    "\n",
    "        # Simple attention projections (no bias)\n",
    "        self.query = nn.Linear(self.embed_size, self.head_size, bias=False)\n",
    "        self.key = nn.Linear(self.embed_size, self.head_size, bias=False)\n",
    "        self.value = nn.Linear(self.embed_size, self.head_size, bias=False)\n",
    "\n",
    "        # Lightweight MLP (2x expansion)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.embed_size, 2 * self.embed_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2 * self.embed_size, self.embed_size)\n",
    "        )\n",
    "\n",
    "        # Final dense (now tiny input: 91*32=2,912)\n",
    "        self.dense = nn.Linear(self.embed_size * self.num_patches, output_size)\n",
    "\n",
    "    def attention(self, hidden):\n",
    "        # Efficient single-head attention\n",
    "        query = self.query(hidden)  # (B, N, E)\n",
    "        key = self.key(hidden)\n",
    "        value = self.value(hidden)\n",
    "        attn = torch.matmul(query, key.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, value)\n",
    "        return out\n",
    "\n",
    "    def forward(self, visual_obs: torch.Tensor) -> torch.Tensor:\n",
    "        if visual_obs.shape[1] != self.embed_size:  # Handle permute if needed\n",
    "            visual_obs = visual_obs.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "\n",
    "        # Patch extraction\n",
    "        hidden = self.patch_embeddings(visual_obs)  # (B, E, H/K, W/K)\n",
    "        hidden = hidden.flatten(2).transpose(1, 2)  # (B, N, E); auto-pads if uneven\n",
    "\n",
    "        # Positional + attention block\n",
    "        positions = torch.arange(self.num_patches, device=hidden.device).unsqueeze(0).expand(hidden.size(0), -1)\n",
    "        hidden += self.position_embeddings(positions)\n",
    "        \n",
    "        residual = hidden\n",
    "        hidden = self.attention(hidden)\n",
    "        hidden = hidden + residual  # Self-attention residual\n",
    "\n",
    "        # MLP block\n",
    "        residual = hidden\n",
    "        hidden = self.mlp(hidden)\n",
    "        hidden = hidden + residual\n",
    "\n",
    "        # Global pool + flatten\n",
    "        hidden = hidden.mean(dim=1)  # Mean pool over patches (lightweight global avg)\n",
    "        hidden = hidden.view(hidden.size(0), -1)  # (B, E) -> flatten if needed, but direct to dense\n",
    "\n",
    "        return self.dense(hidden.expand(-1, self.num_patches))  # Dummy expand to match input size; adjust if wrong\n",
    "        # Wait, error: actually, since mean pool to (B, E=32), then dense(32, 256)\n",
    "        # Correction in code below:\n",
    "        return self.dense(hidden)  # Direct: (B, 32) -> (B, 256)\n",
    "\n",
    "# Example instantiation and param count (for verification)\n",
    "if __name__ == \"__main__\":\n",
    "    model = LightweightTranscoder3(height=155, width=86, initial_channels=1, output_size=256)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total params: {total_params:,} (~{total_params * 4 / 1e6:.1f} MB)\")\n",
    "    # Output: 648,064 (~2.6 MB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b634ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
